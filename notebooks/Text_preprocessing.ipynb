{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58348b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries used\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize, RegexpTokenizer, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import pandas as pd\n",
    "from nltk.probability import FreqDist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0139ac18",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "### a. To implement operations such as:\n",
    "#### Change of case, sentence tokenization, word tokenization, stop word removal, punctuation mark removal, stemming, lemmatization, Parts of Speech (PoS) tagging using NLTK (Natural Language Tool Kit) platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20e0dfd",
   "metadata": {},
   "source": [
    "### Change of case:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7aafe954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a text\n"
     ]
    }
   ],
   "source": [
    "a = 'This iS a TeXt' \n",
    "# Text is not uniform since it has characters with varied cases\n",
    "# and hence should be converted to uniform case (in this case Lower case)\n",
    "# for conducting operations on it.\n",
    "a = a.lower()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23ddd48",
   "metadata": {},
   "source": [
    "#### Sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e62aeda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a demo text used for testing the various built in methods of nltk library.', 'This is a new sentence.']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"This is a demo text used for testing the various built in methods of nltk library. This is a new sentence.\"\n",
    "print(sent_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd404e3",
   "metadata": {},
   "source": [
    "#### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e55b9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'demo', 'text', 'used', 'for', 'testing', 'the', 'various', 'built', 'in', 'methods', 'of', 'nltk', 'library']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"This is a demo text used for testing the various built in methods of nltk library\"\n",
    "print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c5555bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\leosr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa46a0ed",
   "metadata": {},
   "source": [
    "#### Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "096916dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'demo', 'text', 'used', 'testing', 'various', 'built', 'methods', 'nltk', 'library']\n"
     ]
    }
   ],
   "source": [
    "stopword = stopwords.words('english')\n",
    "text = \"This is a demo text used for testing the various built in methods of nltk library\"\n",
    "word_tokens = nltk.word_tokenize(text)\n",
    "removing_stopwords = [word for word in word_tokens if word not in stopword]\n",
    "print (removing_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c70b2f",
   "metadata": {},
   "source": [
    "#### Punctuation mark Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "abe49ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'demo', 'text', 'used', 'for', 'testing', 'the', 'various', 'doesn', 't', 'and', 'built', 'in', 'methods', 'of', 'nltk', 'library']\n"
     ]
    }
   ],
   "source": [
    "text = \"This' is' a demo text used for testing the various, doesn't, and, built in methods of nltk library\"\n",
    "# Create a tokenize based on a regular expression.\n",
    "# \"[a-zA-Z0-9]+\" captures all alphanumeric characters\n",
    "tokenizer = RegexpTokenizer(r\"[a-zA-Z0-9]+\")\n",
    "# Tokenize str1\n",
    "words1 = tokenizer.tokenize(text)\n",
    "print(words1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7f9c69",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "97221460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play\n",
      "play\n",
      "play\n",
      "play\n"
     ]
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "print(porter.stem(\"play\"))\n",
    "print(porter.stem(\"playing\"))\n",
    "print(porter.stem(\"plays\"))\n",
    "print(porter.stem(\"played\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66c3e06",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "91bcc7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play\n",
      "play\n",
      "play\n",
      "play\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"plays\", 'v'))\n",
    "print(lemmatizer.lemmatize(\"played\", 'v'))\n",
    "print(lemmatizer.lemmatize(\"play\", 'v'))\n",
    "print(lemmatizer.lemmatize(\"playing\", 'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f5ec26",
   "metadata": {},
   "source": [
    "#### Part of Speech Tagging (PoS Tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a56db839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('demo', 'NN'), ('text', 'NN'), ('used', 'VBN'), ('for', 'IN'), ('testing', 'VBG'), ('the', 'DT'), ('various', 'JJ'), ('built', 'VBN'), ('in', 'IN'), ('methods', 'NNS'), ('of', 'IN'), ('nltk', 'JJ'), ('library', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a demo text used for testing the various built in methods of nltk library\"\n",
    "tokenized_text = word_tokenize(text)\n",
    "tags = tokens_tag = pos_tag(tokenized_text)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d97be4c",
   "metadata": {},
   "source": [
    "### b. To implement tokenization without using built in function of nltk.\n",
    "\n",
    "We take a given text in the form of a sentence and split it using a particular parameter such as comma (,) (which is popular in csv files), or just normal space ( ) which is used in general text format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aae821b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'demo', 'text', 'used', 'for', 'testing', 'the', 'various', 'built', 'in', 'methods', 'of', 'nltk', 'library']\n",
      "\n",
      "['This', ' is', ' a demo text ', 'used for testing', ' the various built', ' in methods of nltk library']\n"
     ]
    }
   ],
   "source": [
    "# Performing tokenization in a sentence using space as a parameter\n",
    "text = \"This is a demo text used for testing the various built in methods of nltk library\"\n",
    "print(text.split())\n",
    "\n",
    "print()\n",
    "\n",
    "# Performing tokenization in a sentence using comma as a parameter\n",
    "text = \"This, is, a demo text ,used for testing, the various built, in methods of nltk library\"\n",
    "print(text.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be1f98f",
   "metadata": {},
   "source": [
    "#### c. To comprehend the difference between stemming and lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbcc49d",
   "metadata": {},
   "source": [
    "#### Stemming and lemmatization are both techniques used in Natural Language Processing (NLP) to reduce words to their base form. The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. However, the two words differ in their flavor. Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma 12.\n",
    "\n",
    "#### In other words, stemming is faster than lemmatization because it just gets the origin or root/base word unlike lemmatization which makes sense for the word 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a345df6d",
   "metadata": {},
   "source": [
    "#### d. To count frequency of each word in the given document (using nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cb9557fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This: 1\n",
      "a: 1\n",
      "built: 1\n",
      "demo: 1\n",
      "for: 1\n",
      "in: 1\n",
      "is: 1\n",
      "library: 1\n",
      "methods: 1\n",
      "nltk: 1\n",
      "of: 1\n",
      "testing: 1\n",
      "text: 1\n",
      "the: 1\n",
      "used: 1\n",
      "various: 1\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a demo text used for testing the various built in methods of nltk library\"\n",
    "\n",
    "wt_words = text.split()\n",
    "data_analysis = nltk.FreqDist(wt_words)\n",
    "\n",
    "filter_words = dict([(m, n) for m, n in data_analysis.items()])\n",
    "\n",
    "for key in sorted(filter_words):\n",
    "    print(\"%s: %s\" % (key, filter_words[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7064ea0",
   "metadata": {},
   "source": [
    "#### d. To count frequency of each word in the given document (without using nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "022101a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'To': 1, 'know': 4, 'that': 3, 'we': 4, 'what': 2, 'know,': 2, 'and': 1, 'to': 1, 'do': 2, 'not': 2, 'is': 1, 'true': 1, 'knowledge.': 1}\n"
     ]
    }
   ],
   "source": [
    "d = {}\n",
    "file = open(\"textfile.txt\", 'r')\n",
    "for i in file.read().split():\n",
    "    if i in d:\n",
    "        d[i] += 1\n",
    "    else:\n",
    "        d[i] = 1\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee69ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
